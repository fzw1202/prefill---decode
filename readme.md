在这个lab中，你将基于transformers实现一个简单的推理系统。

你需要在autodl上的北京B区创建2卡4090的实例，模型在公共文件存储中。

作为lab的开始，你可以看到 `base_app` ，在这个代码中，你已经得到了推理系统的接收请求和回复的部分。

lab的测试，在终端中运行
```
python3 base_app.py  #后续可以是python first_part.py/ python second_part.py
```
之后，你的推理系统将在服务器的8000端口监控大模型推理请求

这时候你可以使用`curl`指令访问系统得到回复

```
curl -X POST "http://127.0.0.1:8000/generate" -H "Content-Type: application/json" -d '{
    "prompt": "你是谁",
    "temperature": 0.7,
    "min_length": 50,
    "max_length": 500
    }'
```

作为推理系统优化的主要目标之一，我们将每个请求的回复时间作为最终评测的指标。

运行推理服务之后，你可以通过 `./test`来测试你的系统目前针对一些简单数据的响应时间。

运行`./test`，你可以看到类似下面的输出

```
请求 0 完成，延时 71.42 秒
请求 1 完成，延时 71.22 秒
请求 5 完成，延时 115.68 秒
请求 9 完成，延时 167.55 秒
请求 2 完成，延时 180.56 秒
请求 3 完成，延时 178.06 秒
请求 4 完成，延时 177.96 秒
请求 6 完成，延时 174.76 秒
请求 7 完成，延时 170.75 秒
请求 8 完成，延时 167.75 秒
请求 10 完成，延时 181.55 秒
总耗时: 1657.267758846283 秒
```

并在当前文件夹下看到responses.json文件。

通过之后两部分的修改，你将看到上述总耗时逐渐减少。总耗时越短，lab得分越高。

本lab分两个部分，第一部分是对推理系统的改进，第二部分是pd分离的实现。这两部分相对解耦，不构成严格的依赖关系。


### 第一部分

第一个lab，请修改`first_part.py`文件。

现在让我们关注于大模型推理的部分，`base_app` 把每3秒接收到的请求组成一个batch进行推理，这是基础且常见的方法。

但直接调用 `model.generate` 并不是一个好主意。

可以看到代码中
```
outputs = model.generate(  # 使用模型生成文本
    **inputs,
    max_length=max(max_lengths),  # 设置最大长度
    min_length=min(min_lengths),  # 设置最小长度
    temperature=max(temperatures),  # 设置温度参数
    do_sample=True,  # 进行随机采样生成
)
```
对于`max_length`这些参数的处理是粗暴的。

一个我们希望它最长只生成200个token的请求A和另一个希望最长生成20000个token的请求B组成batch时可能会生成大于200个token，这是不符合用户期望的。

所以在lab的第一部分，请你修改大模型推理部分的逻辑，使得你的推理系统可以正确响应每个不同请求的参数要求。

（本lab不对模型的采样算法做要求，大家甚至可以直接贪心，但希望至少模型说的还像是人话）

### 介于一二部分之间的优化

简单的等待3秒组batch显然有很大的优化空间，比如可以实现当前request队列，有结束就踢出队列，新来就插入队列，但整体的序列长度如何控制，如何处理新的prefill和kvcache转移，怎么与第二部分的分离进行结合，都可能是问题。

由于和实际业界中setting的不同，在收到多种限制简化多了的本lab中，pd分离并不一定可以达到和理想中一样的唯一领先效果。

本lab支持同学们在这方面各显神通，也欢迎能够甚至没写pd分离只靠这些技巧就能大大优化推理速度的同学和助教交流心得。

#### 额外要求：大家不要通过修改max_batch_size或更改模型结构(如量化等)来加速

### 第二部分

第二部分请修改 `second_part.py` 文件.

prefill-decode分离已经是目前工业界和学术界公认的推理阶段重要优化。

核心在于让prefill阶段在一部分实例上运行之后，把kvcache传输给另外的只做decode的实例。

再细分下去还有基础的dist serv，结合序列并行的loong serv，结合kvcache缓存池的mooncake，只对长输入请求做分离等方法。

在本lab中，我们只需要实现最简单的分离即可。

在接收到请求之后，你要先在一张gpu上运行prefill，然后将kvcache传输到另一个gpu，另一个gpu收到kvcache之后和它本来正在推理的batch之间进行组合继续推理。

过程看起来简单，但也有很多需要注意的点：

* 同步问题，prefill和decode的过程是异步的，但中间kvcache传输到了之后的组batch过程是同步的。
* kvcache之间的长度需要对齐，插入填充符的同时需要维护attention_mask。
* pd分离和组batch之间的结合也有不同方式，但本lab的要求并不严格，采取正常且合适的方式应该都能得到满分。 



### lab评分

* **50%** 完成正确性的修改，保证你的代码能够正确地给用户提供的参数响应服务。(但不能直接写batchsize=1之类的，所以我们仍然要求你的代码在test的测试中的耗时小于1600秒)

* **80%** 做到一定的优化效果，你的代码在test的测试中的耗时小于1200秒

* **100%** 做到较好的优化效果，你的代码在test的测试中的耗时小于600秒

都是比较宽松的限制(标准代码的1.5倍左右)